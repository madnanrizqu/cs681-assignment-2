{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS 681 Assignment 1 Muhammad Adnan Rizqullah 2403851"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset download\n",
        "In this step goal is to make the dataset ready to be used in the Jupyter notebook runtime. the dataset gets downloaded into the runtime of the Jupyter notebook for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfA6MgapDedZ",
        "outputId": "0230a993-4de8-4c0d-e6ae-07f101701ddd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting download process...\n",
            "aclImdb_v1.tar.gz already exists - skipping\n",
            "imdb_reviews.csv already exists - skipping\n",
            "Download process completed!\n",
            "File aclImdb_v1.tar.gz: 8.1 MB\n",
            "File imdb_reviews.csv: 62.8 MB\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_file(url, destination, force=True):\n",
        "    \"\"\"\n",
        "    Download a file from a URL with a user-friendly progress bar\n",
        "    \"\"\"\n",
        "    if os.path.exists(destination) and not force:\n",
        "        print(f\"Found existing file: {destination}\")\n",
        "        print(\"Skipping download (use force=True to redownload)\")\n",
        "        return True\n",
        "\n",
        "    if \"github.com\" in url and \"/blob/\" in url:\n",
        "        url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
        "\n",
        "    print(f\"Starting download from {url}\")\n",
        "    print(f\"Saving to: {destination}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, \n",
        "                              stream=True,\n",
        "                              headers={'Cache-Control': 'no-cache'},\n",
        "                              timeout=30)\n",
        "        \n",
        "        if response.status_code != 200:\n",
        "            print(f\"Download failed! Server returned code: {response.status_code}\")\n",
        "            return False\n",
        "\n",
        "        file_size = int(response.headers.get('content-length', 0))\n",
        "        os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
        "\n",
        "        with open(destination, 'wb') as f:\n",
        "            with tqdm(total=file_size, unit='MB', unit_scale=True/1024/1024) as pbar:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "\n",
        "        downloaded_size = os.path.getsize(destination)\n",
        "        if downloaded_size < 10000 and file_size > 10000:\n",
        "            print(f\"Warning: File seems too small ({downloaded_size/1024/1024:.1f} MB)\")\n",
        "            return False\n",
        "\n",
        "        print(f\"Successfully downloaded: {downloaded_size/1024/1024:.1f} MB\")\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Download failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Files to download\n",
        "files_to_get = {\n",
        "    \"https://github.com/madnanrizqu/cs681-assignment-1/blob/main/aclImdb_v1.tar.gz\": \"aclImdb_v1.tar.gz\",\n",
        "    \"https://github.com/madnanrizqu/cs681-assignment-1/blob/main/imdb_reviews.csv\": \"imdb_reviews.csv\"\n",
        "}\n",
        "\n",
        "print(\"Starting download process...\")\n",
        "\n",
        "for url, filename in files_to_get.items():\n",
        "    output_path = os.path.join(\"./\", filename)\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"{filename} already exists - skipping\")\n",
        "        continue\n",
        "    \n",
        "    if not download_file(url, output_path):\n",
        "        print(f\"Failed to download {filename}\")\n",
        "\n",
        "print(\"Download process completed!\")\n",
        "# Print summary of downloaded files\n",
        "for filename in files_to_get.values():\n",
        "    path = os.path.join(\"./\", filename)\n",
        "    if os.path.exists(path):\n",
        "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
        "        print(f\"File {filename}: {size_mb:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"File {filename} not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hI0SXgCIkk-8",
        "outputId": "546e5644-4a21-4919-d74c-5502024a6d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files in current directory: ['news.ipynb', '.DS_Store', 'requirements.txt', 'environment.yml', 'imdb.ipynb', 'CS681 Assigntment 1.pdf', 'extracted_imdb', '__pycache__', 'README.md', 'aclImdb_v1.tar.gz', 'Assignment 1.pdf', '.gitignore', 'imdb_reviews.csv', '.git']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "target_dir = './'\n",
        "files = os.listdir(target_dir)\n",
        "os.chdir(target_dir)\n",
        "print(\"Files in current directory:\", files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krcYo9qEyAGT",
        "outputId": "4265d33f-e8be-42a0-aec5-48847c434cb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "imdb_reviews.csv already exists. Skipping processing.\n",
            "Steps in process_imdb_data:\n",
            "1. Extracts positive and negative reviews from train and test directories from the tart file\n",
            "\n",
            "2. Count and print files in each directory:\n",
            "- test/pos\n",
            "- test/neg\n",
            "- train/pos\n",
            "- train/neg\n",
            "\n",
            "3. Process text data:\n",
            "- Reads each review file from all directories\n",
            "- Stores text content, sentiment labels (1 for pos, 0 for neg)\n",
            "- Records split type (train or test)\n",
            "\n",
            "4. Create DataFrame:\n",
            "- Combines texts, labels, and splits into pandas DataFrame\n",
            "- Saves DataFrame to 'imdb_reviews.csv'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import tarfile\n",
        "\n",
        "def extract_specific_dirs(\n",
        "    archive_path, output_dir=\"./extracted_data\", target_dirs=None\n",
        "):\n",
        "    if target_dirs is None:\n",
        "        target_dirs = [\"train/pos\", \"train/neg\", \"test/pos\", \"test/neg\"]\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Extracting files to: {output_path.absolute()}\")\n",
        "\n",
        "    with tarfile.open(archive_path, \"r:gz\") as tar:\n",
        "        for member in tar.getmembers():\n",
        "            for target in target_dirs:\n",
        "                if f\"aclImdb/{target}\" in member.name:\n",
        "                    tar.extract(member, output_path)\n",
        "\n",
        "    print(\"Extraction completed!\")\n",
        "\n",
        "def count_files_in_dir(directory):\n",
        "    total_files = 0\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        total_files += len(files)\n",
        "    return total_files\n",
        "\n",
        "def process_imdb_data():\n",
        "    if not os.path.exists(\"./extracted_imdb\"):\n",
        "        extract_specific_dirs(\"./aclImdb_v1.tar.gz\", output_dir=\"./extracted_imdb\")\n",
        "\n",
        "    print(\n",
        "        \"Num of positive in test: \",\n",
        "        count_files_in_dir(\"./extracted_imdb/aclImdb/test/pos\"),\n",
        "    )\n",
        "    print(\n",
        "        \"Num of negative in test: \",\n",
        "        count_files_in_dir(\"./extracted_imdb/aclImdb/test/neg\"),\n",
        "    )\n",
        "    print(\n",
        "        \"Num of positive in train: \",\n",
        "        count_files_in_dir(\"./extracted_imdb/aclImdb/train/pos\"),\n",
        "    )\n",
        "    print(\n",
        "        \"Num of negative in train: \",\n",
        "        count_files_in_dir(\"./extracted_imdb/aclImdb/train/neg\"),\n",
        "    )\n",
        "\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for split in [\"train\", \"test\"]:\n",
        "        for sentiment in [\"pos\", \"neg\"]:\n",
        "            path = f\"./extracted_imdb/aclImdb/{split}/{sentiment}\"\n",
        "            for file in os.listdir(path):\n",
        "                with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                    texts.append(f.read())\n",
        "                    labels.append(1 if sentiment == \"pos\" else 0)\n",
        "\n",
        "    df = pd.DataFrame({\"text\": texts, \"label\": labels })\n",
        "    print(f\"Total samples in DataFrame: {len(df)}\")\n",
        "    df.to_csv(\"imdb_reviews.csv\", index=False)\n",
        "\n",
        "if not os.path.exists(\"imdb_reviews.csv\"):\n",
        "    process_imdb_data()\n",
        "else:\n",
        "    print(\"imdb_reviews.csv already exists. Skipping processing.\")\n",
        "    print(\"\"\"Steps in process_imdb_data:\n",
        "1. Extracts positive and negative reviews from train and test directories from the tart file\n",
        "\n",
        "2. Count and print files in each directory:\n",
        "- test/pos\n",
        "- test/neg\n",
        "- train/pos\n",
        "- train/neg\n",
        "\n",
        "3. Process text data:\n",
        "- Reads each review file from all directories\n",
        "- Stores text content, sentiment labels (1 for pos, 0 for neg)\n",
        "- Records split type (train or test)\n",
        "\n",
        "4. Create DataFrame:\n",
        "- Combines texts, labels, and splits into pandas DataFrame\n",
        "- Saves DataFrame to 'imdb_reviews.csv'\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dateset exploration\n",
        "In this step the goal is to know how best to preprocess the dataset. We also want to know whether there are any empty values and whether the dataset has balanced class distribution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YNXTDZqN2uqp",
        "outputId": "67460ae2-ca6d-4647-b0b4-626532f35511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (50000, 2)\n",
            "\n",
            "First 5 rows:\n",
            "                                                text  label\n",
            "0  For a movie that gets no respect there sure ar...      1\n",
            "1  Bizarre horror movie filled with famous faces ...      1\n",
            "2  A solid, if unremarkable film. Matthau, as Ein...      1\n",
            "3  It's a strange feeling to sit alone in a theat...      1\n",
            "4  You probably all already know this by now, but...      1\n",
            "\n",
            "Columns and data types:\n",
            "text     object\n",
            "label     int64\n",
            "dtype: object\n",
            "\n",
            "Missing values per column:\n",
            "text     0\n",
            "label    0\n",
            "dtype: int64\n",
            "\n",
            "Label distribution:\n",
            "label\n",
            "1    25000\n",
            "0    25000\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample review:\n",
            "For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt....\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "csv_path = os.path.join(\"./\", \"imdb_reviews.csv\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nColumns and data types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "if 'label' in df.columns:\n",
        "    print(\"\\nLabel distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.countplot(x='label', data=df)\n",
        "    plt.title('Distribution of labels')\n",
        "    plt.show()\n",
        "\n",
        "text_column = 'text'\n",
        "print(\"\\nSample review:\")\n",
        "print(df[text_column].iloc[0][:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing\n",
        "In this step the goal is to remove unwanted characters from the dataset, the ones that might negatively impact or at least have no value in the machine learning task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn_W8f7MPirG",
        "outputId": "b8035b20-2a65-4fc4-9782-e9eb0feb6d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preprocessing Statistics:\n",
            "Total characters in dataset: 65,471,551\n",
            "Total HTML chars filtered: 1,212,813\n",
            "Percentage of HTML chars: 1.85%\n",
            "Average HTML chars per review: 24.26\n",
            "\n",
            "Examples where HTML was found:\n",
            "\n",
            "Example (Review #3):\n",
            "Total chars: 2596\n",
            "HTML chars removed: 96 (3.70%)\n",
            "Original: It's a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.<br /><br />Based up...\n",
            "Processed: It's a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.Based upon Thomas Ro...\n",
            "\n",
            "Example (Review #6):\n",
            "Total chars: 648\n",
            "HTML chars removed: 72 (11.11%)\n",
            "Original: You're using the IMDb.<br /><br />You've given some hefty votes to some of your favourite films.<br /><br />It's something you enjoy doing.<br /><br />And it's all because of this. Fifty seconds. One ...\n",
            "Processed: You're using the IMDb.You've given some hefty votes to some of your favourite films.It's something you enjoy doing.And it's all because of this. Fifty seconds. One world ends, another begins.How can i...\n",
            "\n",
            "Example (Review #8):\n",
            "Total chars: 2792\n",
            "HTML chars removed: 48 (1.72%)\n",
            "Original: Made after QUARTET was, TRIO continued the quality of the earlier film versions of the short stories by Maugham. Here the three stories are THE VERGER, MR. KNOW-IT-ALL, and SANITORIUM. The first two a...\n",
            "Processed: Made after QUARTET was, TRIO continued the quality of the earlier film versions of the short stories by Maugham. Here the three stories are THE VERGER, MR. KNOW-IT-ALL, and SANITORIUM. The first two a...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def html_remover(text):\n",
        "    total_chars = len(text)\n",
        "    text_no_html = re.sub('<[^>]*>', '', text)\n",
        "    html_chars_removed = total_chars - len(text_no_html)\n",
        "    return text_no_html, html_chars_removed, total_chars\n",
        "\n",
        "results = df['text'].apply(html_remover)\n",
        "\n",
        "html_stats = pd.DataFrame()\n",
        "\n",
        "df['no_html'] = results.apply(lambda x: x[0])\n",
        "html_stats['html_filtered_count'] = results.apply(lambda x: x[1])\n",
        "html_stats['total_chars'] = results.apply(lambda x: x[2])\n",
        "html_stats.index = df.index\n",
        "\n",
        "print(\"\\nPreprocessing Statistics:\")\n",
        "print(f\"Total characters in dataset: {html_stats['total_chars'].sum():,}\")\n",
        "print(f\"Total HTML chars filtered: {html_stats['html_filtered_count'].sum():,}\")\n",
        "print(f\"Percentage of HTML chars: {(html_stats['html_filtered_count'].sum() / html_stats['total_chars'].sum() * 100):.2f}%\")\n",
        "print(f\"Average HTML chars per review: {html_stats['html_filtered_count'].mean():.2f}\")\n",
        "\n",
        "print(\"\\nExamples where HTML was found:\")\n",
        "html_rows = html_stats[html_stats['html_filtered_count'] > 0].head(3)\n",
        "if len(html_rows) > 0:\n",
        "    for idx in html_rows.index:\n",
        "        print(f\"\\nExample (Review #{idx}):\")\n",
        "        print(f\"Total chars: {html_stats.loc[idx, 'total_chars']}\")\n",
        "        print(f\"HTML chars removed: {html_stats.loc[idx, 'html_filtered_count']} ({(html_stats.loc[idx, 'html_filtered_count'] / html_stats.loc[idx, 'total_chars'] * 100):.2f}%)\")\n",
        "        print(f\"Original: {df.loc[idx, 'text'][:200]}...\")\n",
        "        print(f\"Processed: {df.loc[idx, 'no_html'][:200]}...\")\n",
        "else:\n",
        "    print(\"No HTML tags found in the dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzWeu1rxRd0F",
        "outputId": "41a3b2f4-18b5-43b0-d836-4ddddba07006"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preprocessing Statistics:\n",
            "Total characters in dataset: 64,258,738\n",
            "Total special chars filtered: 623,938\n",
            "Percentage of special chars: 0.97%\n",
            "Average special chars per review: 12.48\n",
            "\n",
            "Examples where special characters were found:\n",
            "\n",
            "Example (Review #0):\n",
            "Total chars: 284\n",
            "Special chars removed: 2 (0.70%)\n",
            "Original: For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni cha...\n",
            "Processed: For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni cha...\n",
            "\n",
            "Example (Review #1):\n",
            "Total chars: 1033\n",
            "Special chars removed: 25 (2.42%)\n",
            "Original: Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV's \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her atte...\n",
            "Processed: Bizarre horror movie filled with famous faces but stolen by Cristina Raines later of TVs Flamingo Road as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted...\n",
            "\n",
            "Example (Review #3):\n",
            "Total chars: 2500\n",
            "Special chars removed: 20 (0.80%)\n",
            "Original: It's a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.Based upon Thomas Ro...\n",
            "Processed: Its a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.Based upon Thomas Roc...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def special_char_remover(text):\n",
        "    total_chars = len(text)\n",
        "    text_clean = re.sub(r'[^a-zA-Z0-9\\s.,!?-]', '', text)\n",
        "    special_chars_removed = total_chars - len(text_clean)\n",
        "    return text_clean, special_chars_removed, total_chars\n",
        "\n",
        "results = df['no_html'].apply(special_char_remover)\n",
        "\n",
        "char_stats = pd.DataFrame()\n",
        "\n",
        "df['no_html_special_chars'] = results.apply(lambda x: x[0])\n",
        "char_stats['special_chars_removed'] = results.apply(lambda x: x[1])\n",
        "char_stats['total_chars'] = results.apply(lambda x: x[2])\n",
        "char_stats.index = df.index\n",
        "\n",
        "print(\"\\nPreprocessing Statistics:\")\n",
        "print(f\"Total characters in dataset: {char_stats['total_chars'].sum():,}\")\n",
        "print(f\"Total special chars filtered: {char_stats['special_chars_removed'].sum():,}\")\n",
        "print(f\"Percentage of special chars: {(char_stats['special_chars_removed'].sum() / char_stats['total_chars'].sum() * 100):.2f}%\")\n",
        "print(f\"Average special chars per review: {char_stats['special_chars_removed'].mean():.2f}\")\n",
        "\n",
        "print(\"\\nExamples where special characters were found:\")\n",
        "special_rows = char_stats[char_stats['special_chars_removed'] > 0].head(3)\n",
        "if len(special_rows) > 0:\n",
        "    for idx in special_rows.index:\n",
        "        print(f\"\\nExample (Review #{idx}):\")\n",
        "        print(f\"Total chars: {char_stats.loc[idx, 'total_chars']}\")\n",
        "        print(f\"Special chars removed: {char_stats.loc[idx, 'special_chars_removed']} ({(char_stats.loc[idx, 'special_chars_removed'] / char_stats.loc[idx, 'total_chars'] * 100):.2f}%)\")\n",
        "        print(f\"Original: {df.loc[idx, 'no_html'][:200]}...\")\n",
        "        print(f\"Processed: {df.loc[idx, 'no_html_special_chars'][:200]}...\")\n",
        "else:\n",
        "    print(\"No special characters found in the dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/madnanrizqu/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/madnanrizqu/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preprocessing Statistics:\n",
            "Total words in dataset: 12,485,027\n",
            "Total stop words removed: 5,345,643\n",
            "Percentage of stop words: 42.82%\n",
            "Average stop words per review: 106.91\n",
            "\n",
            "Examples of stop words removal:\n",
            "\n",
            "Example (Review #0):\n",
            "Total words: 57\n",
            "Stop words removed: 22 (38.60%)\n",
            "Original: For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni cha...\n",
            "Processed: movie gets respect sure lot memorable quotes listed gem . imagine movie joe piscopo actually funny ! maureen stapleton scene stealer . moroni character absolute scream . watch alan skipper hale jr. po...\n",
            "\n",
            "Example (Review #1):\n",
            "Total words: 181\n",
            "Stop words removed: 63 (34.81%)\n",
            "Original: Bizarre horror movie filled with famous faces but stolen by Cristina Raines later of TVs Flamingo Road as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted...\n",
            "Processed: bizarre horror movie filled famous faces stolen cristina raines later tvs flamingo road pretty somewhat unstable model gummy smile slated pay attempted suicides guarding gateway hell ! scenes raines m...\n",
            "\n",
            "Example (Review #2):\n",
            "Total words: 64\n",
            "Stop words removed: 29 (45.31%)\n",
            "Original: A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physici...\n",
            "Processed: solid , unremarkable film . matthau , einstein , wonderful . favorite part , thing would make go way see , wonderful scene physicists playing badmitton , loved sweaters conversation waited robbins ret...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def stop_words_remover(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    total_words = len(words)\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    stops_removed = total_words - len(filtered_words)\n",
        "    processed_text = ' '.join(filtered_words)\n",
        "    return processed_text, stops_removed, total_words\n",
        "\n",
        "results = df['no_html_special_chars'].apply(stop_words_remover)\n",
        "\n",
        "stops_stats = pd.DataFrame()\n",
        "\n",
        "df['no_html_special_chars_stops'] = results.apply(lambda x: x[0])\n",
        "stops_stats['stops_removed'] = results.apply(lambda x: x[1])\n",
        "stops_stats['total_words'] = results.apply(lambda x: x[2])\n",
        "stops_stats.index = df.index\n",
        "\n",
        "print(\"\\nPreprocessing Statistics:\")\n",
        "print(f\"Total words in dataset: {stops_stats['total_words'].sum():,}\")\n",
        "print(f\"Total stop words removed: {stops_stats['stops_removed'].sum():,}\")\n",
        "print(f\"Percentage of stop words: {(stops_stats['stops_removed'].sum() / stops_stats['total_words'].sum() * 100):.2f}%\")\n",
        "print(f\"Average stop words per review: {stops_stats['stops_removed'].mean():.2f}\")\n",
        "\n",
        "print(\"\\nExamples of stop words removal:\")\n",
        "stop_rows = stops_stats.head(3)\n",
        "for idx in stop_rows.index:\n",
        "    print(f\"\\nExample (Review #{idx}):\")\n",
        "    print(f\"Total words: {stops_stats.loc[idx, 'total_words']}\")\n",
        "    print(f\"Stop words removed: {stops_stats.loc[idx, 'stops_removed']} ({(stops_stats.loc[idx, 'stops_removed'] / stops_stats.loc[idx, 'total_words'] * 100):.2f}%)\")\n",
        "    print(f\"Original: {df.loc[idx, 'no_html_special_chars'][:200]}...\")\n",
        "    print(f\"Processed: {df.loc[idx, 'no_html_special_chars_stops'][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MQBOEw3ze4Gk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Vectorization Statistics:\n",
            "Number of documents: 50000\n",
            "Vocabulary size: 20000\n",
            "Total words: 5,805,122\n",
            "\n",
            "Top 10 most frequent terms:\n",
            "         term  count\n",
            "11819   movie  86993\n",
            "6860     film  77615\n",
            "12516     one  53009\n",
            "10503    like  40113\n",
            "7749     good  29674\n",
            "18089    time  25033\n",
            "6297     even  24832\n",
            "19796   would  24217\n",
            "14407  really  23082\n",
            "15721     see  22981\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=20000,\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(df['no_html_special_chars_stops'])\n",
        "\n",
        "vocab_df = pd.DataFrame(\n",
        "    X.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "print(\"\\nVectorization Statistics:\")\n",
        "print(f\"Number of documents: {X.shape[0]}\")\n",
        "print(f\"Vocabulary size: {X.shape[1]}\")\n",
        "print(f\"Total words: {X.sum():,}\")\n",
        "\n",
        "word_counts = np.asarray(X.sum(axis=0)).ravel()\n",
        "word_freq = pd.DataFrame({\n",
        "    'term': vectorizer.get_feature_names_out(),\n",
        "    'count': word_counts\n",
        "})\n",
        "print(\"\\nTop 10 most frequent terms:\")\n",
        "print(word_freq.sort_values('count', ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset split\n",
        "In this step we want to have a split where more data is allocated to the training set but enough data is there to evaluate the model properly in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of X_train: 40000\n",
            "Length of y_train: 40000\n",
            "Length of X_test: 10000\n",
            "Length of y_test: 10000\n",
            "\n",
            "Labels in y_train:\n",
            "label\n",
            "1    20035\n",
            "0    19965\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Labels in y_test:\n",
            "label\n",
            "0    5035\n",
            "1    4965\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['no_html_special_chars_stops']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "print(f\"Length of X_train: {len(X_train)}\")\n",
        "print(f\"Length of y_train: {len(y_train)}\")\n",
        "print(f\"Length of X_test: {len(X_test)}\")\n",
        "print(f\"Length of y_test: {len(y_test)}\")\n",
        "\n",
        "print(\"\\nLabels in y_train:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"\\nLabels in y_test:\")\n",
        "print(y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vectorization\n",
        "In this step we transform the text into numerical representations that can be consumed by the computer and by extension the machine learning algorithms.\n",
        "\n",
        "## Model training\n",
        "In this step the goal is to train Logistic Regression and Naive Bayes for task classification. For the IMDB dataset we particularly do Sentiment Analysis.\n",
        "\n",
        "## Model testing\n",
        "In this step the goal is to find out the effects of vector embedding choice with model accuray on task classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training word2vec for later steps...\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(\"Training word2vec for later steps...\")\n",
        "\n",
        "tokenized_train_texts = [word_tokenize(text.lower()) for text in X_train]\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_train_texts,\n",
        "    vector_size=300, \n",
        "    window=5,        \n",
        "    min_count=2,      \n",
        "    sg=1,            # use skip-gram\n",
        "    workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting Logistic Regression to dataset...\n",
            "Fitting Logistic Regression with TF-IDF features...\n",
            "Fitting Logistic Regression with TF-IDF features...\n",
            "Generating word2vec representation of training dataset...\n",
            "Fitting Logistic Regression with Word2Vec features...\n",
            "\n",
            "Model Comparison:\n",
            "CountVectorizer + LogisticRegression accuracy: 0.8848\n",
            "TF-IDF + LogisticRegression accuracy: 0.9007\n",
            "Word2Vec + LogisticRegression accuracy: 0.8787\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=20000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "print(\"Fitting Logistic Regression to dataset...\")\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=20000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_model = LogisticRegression(max_iter=200)\n",
        "print(\"Fitting Logistic Regression with TF-IDF features...\")\n",
        "tfidf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "\n",
        "tfidf_model = LogisticRegression(max_iter=200)\n",
        "print(\"Fitting Logistic Regression with TF-IDF features...\")\n",
        "tfidf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_tfidf = tfidf_model.predict(X_test_tfidf)\n",
        "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
        "\n",
        "def get_doc_vector(text, model):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "    \n",
        "print(\"Generating word2vec representation of training dataset...\")\n",
        "X_train_w2v = np.array([get_doc_vector(text, w2v_model) for text in X_train])\n",
        "X_test_w2v = np.array([get_doc_vector(text, w2v_model) for text in X_test])\n",
        "\n",
        "w2v_model_lr = LogisticRegression(max_iter=200)\n",
        "print(\"Fitting Logistic Regression with Word2Vec features...\")\n",
        "w2v_model_lr.fit(X_train_w2v, y_train)\n",
        "\n",
        "y_pred_w2v = w2v_model_lr.predict(X_test_w2v)\n",
        "accuracy_w2v = accuracy_score(y_test, y_pred_w2v)\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(f\"CountVectorizer + LogisticRegression accuracy: {accuracy:.4f}\")\n",
        "print(f\"TF-IDF + LogisticRegression accuracy: {accuracy_tfidf:.4f}\")\n",
        "print(f\"Word2Vec + LogisticRegression accuracy: {accuracy_w2v:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting MultinomialNB to dataset...\n",
            "Fitting MultinomialNB with TF-IDF features...\n",
            "Fitting MultinomialNB with Word2Vec features...\n",
            "\n",
            "Model Comparison:\n",
            "CountVectorizer + MultinomialNB accuracy: 0.8578\n",
            "TF-IDF + MultinomialNB accuracy: 0.8648\n",
            "Word2Vec + MultinomialNB accuracy: 0.7638\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=20000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "model = MultinomialNB()\n",
        "print(\"Fitting MultinomialNB to dataset...\")\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_vec)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=20000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_model = MultinomialNB()\n",
        "print(\"Fitting MultinomialNB with TF-IDF features...\")\n",
        "tfidf_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_tfidf = tfidf_model.predict(X_test_tfidf)\n",
        "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
        "\n",
        "def get_doc_vector(text, model):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    if vectors:\n",
        "        vec = np.mean(vectors, axis=0)\n",
        "        vec = vec - vec.min()\n",
        "        return vec\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "X_train_w2v_nb = np.array([get_doc_vector(text, w2v_model) for text in X_train])\n",
        "X_test_w2v_nb = np.array([get_doc_vector(text, w2v_model) for text in X_test])\n",
        "\n",
        "w2v_model_nb = MultinomialNB()\n",
        "print(\"Fitting MultinomialNB with Word2Vec features...\")\n",
        "w2v_model_nb.fit(X_train_w2v_nb, y_train)\n",
        "\n",
        "y_pred_w2v = w2v_model_nb.predict(X_test_w2v_nb)\n",
        "accuracy_w2v = accuracy_score(y_test, y_pred_w2v)\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(f\"CountVectorizer + MultinomialNB accuracy: {accuracy:.4f}\")\n",
        "print(f\"TF-IDF + MultinomialNB accuracy: {accuracy_tfidf:.4f}\")\n",
        "print(f\"Word2Vec + MultinomialNB accuracy: {accuracy_w2v:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cs681",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
